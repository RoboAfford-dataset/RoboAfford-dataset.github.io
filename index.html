<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="RoboAfford: A Dataset and Benchmark for Enhancing Object and Spatial Affordance Learning in Robot Manipulation">
  <meta property="og:title" content="RoboAfford"/>
  <meta property="og:description" content="A Dataset and Benchmark for Enhancing Object and Spatial Affordance Learning in Robot Manipulation"/>
  <meta property="og:url" content="https://github.com/tyb197/RoboAfford"/>
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
  <meta name="twitter:title" content="RoboAfford">
  <meta name="twitter:description" content="A Dataset and Benchmark for Enhancing Object and Spatial Affordance Learning in Robot Manipulation">
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="Robot Manipulation, Object Affordance, Spatial Affordance">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>RoboAfford: A Dataset and Benchmark for Enhancing Object and Spatial Affordance Learning in Robot Manipulation</title>
  <link rel="icon" type="image/x-icon" href="static/images/robot.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">RoboAfford: A Dataset and Benchmark for Enhancing Object and Spatial Affordance Learning in Robot Manipulation</h1>
          <div class="is-size-5 publication-authors" style="margin-top: 1rem;">
  <span class="author-block">
    <a href="#">Yingbo Tang</a><sup>1,2*</sup>,
    <a href="#">Lingfeng Zhang</a><sup>3,*</sup>,
    <a href="#">Shuyi Zhang</a><sup>1,2</sup>,
    <a href="#">Yinuo Zhao</a><sup>4</sup>,
    <a href="#">Xiaoshuai Hao</a><sup>5,†</sup>
  </span>
  <br>
  <span class="author-block">
    <sup>1</sup>Institute of Automation, Chinese Academy of Sciences<br>
    <sup>2</sup>School of Artiffcial Intelligence, University of Chinese Academy of Sciences<br>
    <sup>3</sup>Shenzhen International Graduate School, Tsinghua University<br>
    <sup>4</sup>Beijing Institute of Technology<br>
    <sup>5</sup>Beijing Academy of Artificial Intelligence (BAAI)
  </span>
  <br>
  <span class="author-block">
    <sup>*</sup>Co-first Authors &nbsp;&nbsp;
    <sup>†</sup>Corresponding Author
  </span>
</div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="static/pdfs/RoboAfford.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
                </a>
              </span>
<!--               <span class="link-block">
                <a href="static/pdfs/supplementary_material.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Supplementary</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://github.com/tyb197/RoboAfford" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span><span>Code</span>
                </a>
              </span>
<!--               <span class="link-block">
                <a href="https://arxiv.org/abs/XXXX.XXXXX" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
                </a>
              </span> -->
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/tyb197/RoboAfford" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="far fa-images"></i></span><span>Dataset</span>
                  </a>
                </span>
              <!-- Benchmark Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/tyb197/RoboAfford-Eval" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="far fa-images"></i></span><span>Benchmark</span>
                  </a>
                </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

  <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
          <img 
              src="static/images/figure1.png"
              class="pipeline image"
              alt="pipeline image"
              style="margin-bottom: 20px;"
          />
      <h2 class="subtitle has-text-centered">
         This work introduces <strong>RoboAfford</strong>, a novel large-scale dataset designed to enhance object and spatial affordance learning in robot manipulation. It encompasses three key dimensions: object affordance recognition, object affordance prediction, and spatial affordance localization.
      </h2>
    </div>
  </div>
</section>
  
<!-- Abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Robot manipulation is a fundamental capability of embodied intelligence, enabling effective robot interactions with the physical world. 
            In robot manipulation tasks, predicting precise grasping positions and object placement is essential. 
            Achieving this requires <strong>object recognition</strong> to localize target object, predicting <strong>object affordances</strong> for interaction and <strong>spatial affordances</strong> for optimal arrangement. 
            While Vision-Language Models (VLMs) provide insights for high-level task planning and scene understanding, they often struggle to predict precise action positions, such as functional grasp points and spatial placements. 
            This limitation stems from the lack of annotations for object and spatial affordance data in their training datasets.
          </p>
          <p>
            To address this gap, we introduce <strong>RoboAfford</strong>, a novel large-scale dataset designed to enhance object and spatial affordance learning in robot manipulation. 
            Our dataset comprises 819,987 images paired with 1.9 million question answering (QA) annotations, covering three critical tasks: <strong>object affordance recognition</strong> to identify objects based on attributes and spatial relationships, <strong>object affordance prediction</strong> to pinpoint functional grasping parts, and <strong>spatial affordance localization</strong> to identify free space for placement.
            Complementing this dataset, we propose <strong>RoboAfford-Eval</strong>, a comprehensive benchmark for assessing affordance-aware prediction in real-world scenarios, featuring 338 meticulously annotated samples across the same three tasks.
            Extensive experimental results reveal the deficiencies of existing VLMs in affordance learning, while fine-tuning on the <strong>RoboAfford</strong> dataset significantly enhances their affordance prediction in robot manipulation, validating the dataset's effectiveness.
            The dataset, benchmark and evaluation code will be made publicly available to facilitate future research.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Abstract -->

  
<!-- Data Construction Pipeline -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="title is-3">Dataset Construction</h2>
            <img src="static/images/data_construction.png" alt="NaVid" class="center-image blend-img-background">
            <div class="level-set has-text-justified">
              <p class="has-text-justified">
               Pipeline for constructing the RoboAfford dataset. We first discard the image with densely repeated objects, and then generate question answering pairs using human designed template or GPT-4o.
              </p>
            </div>
          </div>
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="subtitle is-4">Comparison of Existing Affordance Datasets</h2>
            <img src="static/images/table1.png" alt="NaVid" class="center-image blend-img-background">
            <div class="level-set has-text-justified">
              <p class="has-text-justified">
                Obj-Aff: Object Affordance. Spa-Aff: Spatial Affordance.
              </p>
            </div>
          </div>
        </div>
      </div>

      
    </div>
  </div>
</section>
<!-- End Data Construction Pipeline -->



<!-- Methodology -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="title is-3">Methodology</h2>
            <img src="static/images/figure_framework.png" alt="NaVid" class="center-image blend-img-background">
            <div class="level-set has-text-justified">
              <p class="has-text-justified">
               Our RoboAfford-Qwen Framework. We fine-tune the model on the RoboAfford dataset to enhance object and spatial affordance capabilities. 
              For robotic manipulation, we use depth information to transform 2D points representing objects and spatial affordances into 3D coordinates, which are then converted to end-effector positions for robotic manipulation.
              </p>
            </div>
          </div>
        </div>
      </div>

     </div>
  </div>
</section>
<!-- End Methodology --> 


<!-- Experiments -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="title is-3">Experimental Results</h2>
            <br>
            <h2 class="subtitle is-4">Comparison results of various VLMs on RoboAfford-Eval benchmark.</h2>
            <img src="static/images/table_results.png" alt="NaVid" class="center-image blend-img-background">
            <div class="level-set has-text-justified">
              <p class="has-text-justified">
                
              </p>
            </div>
          </div>
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="title is-4">Real-world Experiments</h2>
            <img src="static/images/figure_results.png" alt="NaVid" class="center-image blend-img-background">
            <div class="level-set has-text-justified">
              <p class="has-text-justified">
                Qualitative results of RoboAfford-Qwen, where cyan points indicate the object and spatial affordances.
              </p>
            </div>
          </div>
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="subtitle is-4">Robot Manipulation</h2>
            <img src="static/images/figure_franka.png" alt="NaVid" class="center-image blend-img-background">
            <div class="level-set has-text-justified">
              <p class="has-text-justified">
                Results of deploying RoboAfford-Qwen to downstream robotic manipulation tasks.
              </p>
            </div>
          </div>
        </div>
      </div>

    </div>
  </div>
</section>
<!-- End Experiments -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">License</h2>
    <p>The datasets and benchmarks are under the Creative Commons Attribution 4.0 International License.</p>
  </div>
</section>

<!-- You can now continue with your video, carousel, BibTeX, etc. -->

 
