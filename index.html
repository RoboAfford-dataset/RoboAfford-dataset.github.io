<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="RoboAfford: A Dataset and Benchmark for Enhancing Object and Spatial Affordance Learning in Robot Manipulation">
  <meta property="og:title" content="RoboAfford"/>
  <meta property="og:description" content="A Dataset and Benchmark for Enhancing Object and Spatial Affordance Learning in Robot Manipulation"/>
  <meta property="og:url" content="https://github.com/tyb197/RoboAfford"/>
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
  <meta name="twitter:title" content="RoboAfford">
  <meta name="twitter:description" content="A Dataset and Benchmark for Enhancing Object and Spatial Affordance Learning in Robot Manipulation">
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="Robot Manipulation, Object Affordance, Spatial Affordance">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>RoboAfford: A Dataset and Benchmark for Enhancing Object and Spatial Affordance Learning in Robot Manipulation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">RoboAfford: A Dataset and Benchmark for Enhancing Object and Spatial Affordance Learning in Robot Manipulation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Yingbo Tang, Lingfeng Zhang, Shuyi Zhang, Yinuo Zhao, Xiaoshuai Hao</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Affiliation TBD<br>Conference Name and Year</span>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/XXXX.XXXXX.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="static/pdfs/supplementary_material.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Supplementary</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/tyb197/RoboAfford" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span><span>Code</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/XXXX.XXXXX" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Robot manipulation is a fundamental capability of embodied intelligence, enabling effective robot interactions with the physical world. 
            In robot manipulation tasks, predicting precise grasping positions and object placement is essential. 
            Achieving this requires <strong>object recognition<strong> to localize target object, predicting <strong>object affordances<strong> for interaction and <strong>spatial affordances<strong> for optimal arrangement. 
            While Vision-Language Models (VLMs) provide insights for high-level task planning and scene understanding, they often struggle to predict precise action positions, such as functional grasp points and spatial placements. 
            This limitation stems from the lack of annotations for object and spatial affordance data in their training datasets.
          </p>
          <p>
            To address this gap, we introduce <strong>RoboAfford<strong>, a novel large-scale dataset designed to enhance object and spatial affordance learning in robot manipulation. 
            Our dataset comprises 819,987 images paired with 1.9 million question answering (QA) annotations, covering three critical tasks: <strong>object affordance recognition<strong> to identify objects based on attributes and spatial relationships, <strong>object affordance prediction<strong> to pinpoint functional grasping parts, and <strong>spatial affordance localization<strong> to identify free space for placement.
            Complementing this dataset, we propose <strong>RoboAfford-Eval<strong>, a comprehensive benchmark for assessing affordance-aware prediction in real-world scenarios, featuring 338 meticulously annotated samples across the same three tasks.
            Extensive experimental results reveal the deficiencies of existing VLMs in affordance learning, while fine-tuning on the <strong>RoboAfford<strong> dataset significantly enhances their affordance prediction in robot manipulation, validating the dataset's effectiveness.
            The dataset, benchmark and evaluation code will be made publicly available to facilitate future research.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Abstract -->

<!-- You can now continue with your video, carousel, BibTeX, etc. -->

 
